@article{elnaggar_prottrans_2020,
  author={Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning}, 
  year={2022},
  volume={44},
  number={10},
  pages={7112-7127},
  abstract={Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81%-87%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81%) and membrane versus water-soluble (2-state accuracy Q2=91%). For secondary structure, the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the grammar of the language of life. All our models are available through https://github.com/agemagician/ProtTrans.},
  keywords={Proteins;Training;Amino acids;Task analysis;Databases;Computational modeling;Three-dimensional displays;Computational biology;high performance computing;machine learning;language modeling;deep learning},
  doi={10.1109/TPAMI.2021.3095381},
  ISSN={1939-3539},
  month={Oct},
}

@article{simon_interplm_2025,
	title = {{InterPLM}: discovering interpretable features in protein language models via sparse autoencoders},
	volume = {22},
	issn = {1548-7105}, 
	year = {2025},
	url = {https://doi.org/10.1038/s41592-025-02836-7},
	doi = {10.1038/s41592-025-02836-7},
	abstract = {Despite their success in protein modeling and design, the internal mechanisms of protein language models ({PLMs}) are poorly understood. Here we present a systematic framework to extract and analyze interpretable features from {PLMs} using sparse autoencoders. Training sparse autoencoders on {ESM}-2 embeddings, we identify thousands of interpretable features highlighting biological concepts including binding sites, structural motifs and functional domains. Individual neurons show considerably less conceptual alignment, suggesting {PLMs} store concepts in superposition. This superposition persists across model scales and larger {PLMs} capture more interpretable concepts. Beyond known annotations, {ESM}-2 learns coherent patterns across evolutionarily distinct protein families. To systematically analyze these numerous features, we developed an automated interpretation approach using large language models for feature description and validation. As practical applications, these features can accurately identify missing database annotations and enable targeted steering of sequence generation. Our results show {PLM} representations can be decomposed into interpretable components, demonstrating the feasibility and utility of mechanistically interpreting these models.},
	pages = {2107--2117},
	number = {10},
	journaltitle = {Nature Methods},
	journal = {Nature Methods},
	shortjournal = {Nature Methods},
	author = {Simon, Elana and Zou, James},
	date = {2025-10-01},
}


@article{adams_2025_interprot,
	author = {Adams, Etowah and Bai, Liam and Lee, Minji and Yu, Yiyang and AlQuraishi, Mohammed},
	title = {From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models},
	elocation-id = {2025.02.06.636901},
	year = {2025},
	doi = {10.1101/2025.02.06.636901},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Protein language models (pLMs) are powerful predictors of protein structure and function, learning through unsupervised training on millions of protein sequences. pLMs are thought to capture common motifs in protein sequences, but the specifics of pLM features are not well understood. Identifying these features would not only shed light on how pLMs work, but potentially uncover novel protein biology{\textendash}{\textendash}studying the model to study the biology. Motivated by this, we train sparse autoencoders (SAEs) on the residual stream of a pLM, ESM-2. By characterizing SAE features, we determine that pLMs use a combination of generic features and family-specific features to represent a protein. In addition, we demonstrate how known sequence determinants of properties such as thermostability and subcellular localization can be identified by linear probing of SAE features. For predictive features without known functional associations, we hypothesize their role in unknown mechanisms and provide visualization tools to aid their interpretation. Our study gives a better understanding of the limitations of pLMs, and demonstrates how SAE features can be used to help generate hypotheses for biological mechanisms. We release our code, model weights and feature visualizer.Competing Interest StatementM.A. is a member of the scientific advisory boards of Cyrus Biotechnology, Deep Forest Sciences, Nabla Bio, and Oracle Therapeutics.},
	URL = {https://www.biorxiv.org/content/early/2025/06/18/2025.02.06.636901},
	eprint = {https://www.biorxiv.org/content/early/2025/06/18/2025.02.06.636901.full.pdf},
	journal = {bioRxiv}
}

@misc{parsan2025towardsinterpretableproteinstructureprediction,
  title={Towards Interpretable Protein Structure Prediction with Sparse Autoencoders}, 
  author={Nithin Parsan and David J. Yang and John J. Yang},
  year={2025},
  eprint={2503.08764},
  archivePrefix={arXiv},
  primaryClass={q-bio.BM},
  url={https://arxiv.org/abs/2503.08764}, 
}

@online{lindsey_sparse_2024,
	title = {Sparse Crosscoders for Cross-Layer Features and Model Diffing},
	url = {https://transformer-circuits.pub/2024/crosscoders/index.html},
	author = {Lindsey, Jack and Templeton, Adly and Marcus, Jonathan and Conerly, Thomas and Batson, Joshua and Olah, Christopher},
	urldate = {2025-11-28},
	date = {2024-10-25},
	langid = {english},
	keywords = {{SAES}, improvements},
	file = {Sparse Crosscoders for Cross-Layer Features and Model Diffing:/home/sohrab/Zotero/storage/E9T2NTFX/index.html:text/html},
}
